As to the music generation, there are two popular tasks, monophonic music generation together with polyphonic music generation. In general, music generation task has a difficulty to generate long pieces due to the gradient vanish problem[19]. 

In fact, because there is more information in polyphonic music, this problem is more severe for polyphonic music genera-tion task. Several methods have been applied to model the music such as multi-layer LSTM (Chu et al. [2016]), GAN (Dong et al. [2018], Yang et al. [2017], Hsiao et al. [2018]), and other LSTM based model (Chu et al. [2016], Hadjeres et al. [2016], Jaques et al. [2017], Brunner et al. [2017], Shin et al. [2017], Mao et al. [2018], Zhu et al. [2018], Hsiao et al. [2018]). However, we find these methods yield plain result especially when the dataset is large or the generated se-quence is long. In this situation, the notes in generated music are consisted of some wandering pitches and tempos.

For music generation task, recent research of feedforward model like CNN (Bai et al. [2018], Miller et al. [2018]) gives us a new idea to overcome the music quality problems. They find CNN is more resistant to the gradient vanish, plus its complex structure let this model be an ideal choice to fit a large dataset. But we find CNN structure like (Van Den Oord et al. [2016], Bai et al. [2018]) is not capable to fit large dataset such as Wikifonia (6,675 Lead Sheets). As to a smaller dataset like the Bach’s music (Wu et al. [2017]), we find the music generated by simple CNN or resNet like CNN is lack of overall consistency, CNN models generate lots of resting notes and super long notes like Figure 1.

In fact, unlike the LSTM, CNNs which use shared-weight filters are not suitable for extracting temporal features in music. Different regions of a music sequence have different local statistics, the spatial stationarity assumption of con-volution cannot hold. For example, in music, we cannot move basic music elements arbitrarily like the ‘C Am F G’ chord process is distinctively different from ‘C F Am G’ chord process, however, there is little difference in meaning between ‘I’m a cool man’ and ‘A cool man I’m’. To reme-dy this drawback of CNN, we borrow the idea from face recognition task (Taigman et al. [2014]). We introduce the Locally Connected Convolutional Neural Network (Locally Connected CNN (Gregor and LeCun [2010], Huang et al. [2012])) to sequence generation model and achieved a re-markable effect on 2 music generation tasks. Locally Con-nected CNN neural network outperforms traditional LSTM model both in training performance and speed. We also notice the Local Connected CNN can generate super long sequence and maintain well-organized music structure, the harder the music generation task is (larger dataset, poly-phonic music generation), the improvement is more obvi-ous.

In our paper, we firstly introduce Locally Connected Con-volutional (Local Conv) layer and discuss about why we choose this structure to music generation task. Then we summarize general ways to add Local Conv layer to CNN and we let the Local Conv resNet model as an example. Next, we prove the improvement of Local Conv model by evaluating the experiment’s training result. Moreover, we analyze the music sheets and do a user study to further ensure our Local Conv model’s better result. Lastly, we analyze why Local Conv model performs better in music generation task and use visualization prove our assumption about the Local Conv.

Our contributions are as follows: 
•	We introduce a novel Local Conv CNN structure to boost the CNN’s performance on music generation task. 
•	We apply the aforementioned model to 2 generate sym-bolic music. All of these tasks achieve better results than the classic CNN model and LSTM model.
•	We further analyze why Local Conv generate a better result on sequence generation task. We find Local Conv architecture extends the CNN’s modeling capa-bility on time dimension.
